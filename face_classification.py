# -*- coding: utf-8 -*-
"""face_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yby0gHjeg-o3nacCEbo1-ZDLpzHQ_XBW
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/Smart_Travel_App/Face_detect'

# Define your data directory
data_dir = '/content/drive/MyDrive/Smart_Travel_App/Face_detect'

# Load the dataset using ImageDataGenerator
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2)

# Define image dimensions and batch size
height = 80
width = 80
channels = 3
batch_size = 128
img_shape = (height, width, channels)
img_size = (height, width)

tf.config.run_functions_eagerly(True)

datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# Load the dataset using flow_from_directory
data = datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training')

val_data = datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation')

# Create a more complex model
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=img_shape, pooling='avg')

for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(len(data.class_indices), activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

!pip install --upgrade tensorflow
!pip install --upgrade keras

# Calculate the number of training and validation steps
STEP_SIZE_TRAIN = data.n // data.batch_size
STEP_SIZE_VALID = val_data.n // val_data.batch_size

# Define the custom training loop
def custom_train(model, train_data, val_data, epochs):
    train_loss_results = []
    train_accuracy_results = []
    val_loss_results = []
    val_accuracy_results = []

    for epoch in range(epochs):
        epoch_loss_avg = tf.keras.metrics.Mean()
        epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()

        val_epoch_loss_avg = tf.keras.metrics.Mean()
        val_epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()

        # Training loop
        for images, labels in train_data:
            with tf.GradientTape() as tape:
                predictions = model(images)
                loss_value = tf.losses.CategoricalCrossentropy()(labels, predictions)
            grads = tape.gradient(loss_value, model.trainable_variables)
            model.optimizer.apply_gradients(zip(grads, model.trainable_variables))

            epoch_loss_avg.update_state(loss_value)
            epoch_accuracy.update_state(labels, predictions)

        # Validation loop
        for images, labels in val_data:
            predictions = model(images)
            val_loss_value = tf.losses.CategoricalCrossentropy()(labels, predictions)

            val_epoch_loss_avg.update_state(val_loss_value)
            val_epoch_accuracy.update_state(labels, predictions)

        # Save training history
        train_loss_results.append(epoch_loss_avg.result())
        train_accuracy_results.append(epoch_accuracy.result())
        val_loss_results.append(val_epoch_loss_avg.result())
        val_accuracy_results.append(val_epoch_accuracy.result())

        print(f"Epoch {epoch + 1}/{epochs}, "
              f"Train Loss: {epoch_loss_avg.result():.4f}, "
              f"Train Accuracy: {epoch_accuracy.result() * 100:.2f}%, "
              f"Val Loss: {val_epoch_loss_avg.result():.4f}, "
              f"Val Accuracy: {val_epoch_accuracy.result() * 100:.2f}%")

    return train_loss_results, train_accuracy_results, val_loss_results, val_accuracy_results

# Train the model using the built-in fit method
num_epochs = 10  # Adjust the number of epochs as needed
history = model.fit(
    data,
    validation_data=val_data,
    epochs=num_epochs,
    verbose=1
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(val_data, verbose=1)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Plot training history
plt.figure(figsize=(12, 5))

# Plot training & validation loss values
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')

# Plot training & validation accuracy values
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')

plt.show()

# Plot training history
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.xlabel('Epoch Number')
plt.ylabel('Loss')
plt.plot(history.history['loss'], label='training set')
plt.plot(history.history['val_loss'], label='validation set')
plt.legend()

plt.subplot(1, 2, 2)
plt.xlabel('Epoch Number')
plt.ylabel('Accuracy')
plt.plot(history.history['accuracy'], label='training set')
plt.plot(history.history['val_accuracy'], label='validation set')
plt.legend()

# Save the trained model
model_name = 'face_model.h5'
model.save(model_name, save_format='h5')

# Get class names
class_map = data.class_indices
classes = []
for key in class_map.keys():
    classes.append(key)

# Load the model for prediction
from tensorflow.keras.models import load_model
model_name = 'face_model.h5'
model = load_model(model_name)

def predict_image(filename, model, classes):
    img = tf.keras.preprocessing.image.load_img(filename, target_size=(80, 80))
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_processed = np.expand_dims(img_array, axis=0)
    img_processed /= 255.

    prediction = model.predict(img_processed)
    class_index = np.argmax(prediction)
    class_name = classes[class_index]

    plt.title(f"Predicted Class: {class_name}", size=18, color='red')
    plt.imshow(img_array)
    plt.axis('off')

# Example usage of predict_image
classes = ["Angry", "Happy", "Sad", ...]  # Define your class names here
predict_image('/content/drive/MyDrive/Smart_Travel_App/Face_detect/Happy/13.jpg', model, classes)

# Example usage of predict_image
classes = ["Angry", "Happy", "Sad", ...]  # Define your class names here
predict_image('/content/drive/MyDrive/Smart_Travel_App/Face_detect/Angry/13.jpg', model, classes)

# Example usage of predict_image
classes = ["Angry","Disguist","Fear", "Happy","Neutral", "Sad","Surprise", ...]  # Define your class names here
predict_image('/content/drive/MyDrive/Smart_Travel_App/Face_detect/Sad/12.jpg', model, classes)